{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"rhR-ZUkwJrFn"},"source":["## Problem Statement \n","\n","Building a model that is able to classify customer complaints based on the products/services. Segregating tickets help in the quick resolution of the issue.\n","\n","Since this data is not labelled,applying NMF(Topic Modelling) to analyse patterns and classify tickets into the following five clusters based on their products/services:\n","\n","* Credit card / Prepaid card\n","\n","* Bank account services\n","\n","* Theft/Dispute reporting\n","\n","* Mortgages/loans\n","\n","* Others "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mcgXVNyaLUFS"},"source":["## Steps performed:\n","\n","1.  Data loading\n","\n","2. Text preprocessing\n","\n","3. Exploratory data analysis (EDA)\n","\n","4. Feature extraction\n","\n","5. Topic modelling \n","\n","6. Model building using supervised learning\n","\n","7. Model training and evaluation\n","\n","8. Model inference"]},{"cell_type":"markdown","metadata":{"id":"JuLFIymAL58u"},"source":["## Importing the necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-Q9pqrcJrFr"},"outputs":[],"source":["import json \n","import numpy as np\n","import pandas as pd\n","import re, nltk, spacy, string\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from plotly.offline import plot\n","import plotly.graph_objects as go\n","import plotly.express as px\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from pprint import pprint"]},{"cell_type":"markdown","metadata":{"id":"KtRLCsNVJrFt"},"source":["## Loading the data\n","\n","The data is in JSON format and we need to convert it to a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puVzIf_iJrFt"},"outputs":[],"source":["# Opening JSON file \n","f = open(\"complaints-2021-05-14_08_16.json\")\n","  \n","# returns JSON object as  \n","# a dictionary \n","data = json.load(f)\n","df=pd.json_normalize(data)"]},{"cell_type":"markdown","metadata":{"id":"_xYpH-sAJrFu"},"source":["## Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lf8ufHH5JrFu"},"outputs":[],"source":["# Inspect the dataframe to understand the given data.\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Length of dataframe\n","print(\"Total entries in dataframe is :\",len(df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dwcty-wmJrFw"},"outputs":[],"source":["#print the column names\n","existing_cols=list(df.columns)\n","print(existing_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYCtKXD1JrFw"},"outputs":[],"source":["#Assign new column names\n","new_cols=[cols.replace(\"_source.\",\"\") for cols in existing_cols]\n","#new_cols = [cols.replace(\"_\",\"\",1) if cols[0]==\"_\" for cols in new_cols]\n","df.columns=new_cols\n","print(\"The new column names are : \\n\",list(df.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grQUPFL5JrFx"},"outputs":[],"source":["#Assign nan in place of blanks in the complaints column\n","df[\"complaint_what_happened\"] = df[\"complaint_what_happened\"].apply(lambda x: str(x).strip()).replace('', np.nan)\n","blank_complaints=df['complaint_what_happened'].isna().sum()\n","print(\"The total blank complaints are : \",blank_complaints)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jfxd8VSmJrFy"},"outputs":[],"source":["#Remove all rows where complaints column is nan\n","new_df= df[df[\"complaint_what_happened\"].notna()]\n","new_df.reset_index()\n","print(\"Length of dataframe after removing blank complaints :\",len(new_df))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L944HZpsJrFy"},"source":["## Preparing the text for topic modeling\n","\n","Removing blank complaints and doing following preprocessing steps:\n","\n","* Make the text lowercase\n","* Remove text in square brackets\n","* Remove punctuation\n","* Remove words containing numbers\n","\n","\n","After cleaning operations, performing the following:\n","* Lemmatize texts\n","* Extract POS tags of the lemmatized text and removing all the words which have tags other than NN[tag == \"NN\"].\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qm7SjjSkJrFz"},"outputs":[],"source":["# Function to clean the text and remove all the unnecessary elements.\n","def preprocess_complaints(complaint):\n","    complaint=complaint.lower()\n","    complaint=re.sub(\"\\[.*?\\]\",\"\",complaint)\n","    complaint=re.sub(r'[^\\w\\s]', '', complaint)\n","    complaint=re.sub(r'\\w*\\d\\w*', '', complaint).strip()\n","    return complaint    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgOu8t8HJrFz"},"outputs":[],"source":["#Function to Lemmatize the texts\n","nlp = spacy.load('en_core_web_sm')\n","def lemmatize(complaint):\n","    doc = nlp(complaint)\n","    lemma=[]\n","    for token in doc:\n","        lemma.append(token.lemma_)\n","    lemmatized_complaint=\" \".join(lemma)\n","    return lemmatized_complaint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXnN7aa_JrF0"},"outputs":[],"source":["#Creating a dataframe('df_clean') that will have only the complaints and the lemmatized complaints \n","from tqdm import tqdm\n","df_clean = new_df[[\"_id\",\"complaint_what_happened\"]].copy()\n","# Cleaning complaints\n","df_clean[\"cleaned_complaints\"] = df_clean[\"complaint_what_happened\"].apply(preprocess_complaints)\n","lemm_complaints = []\n","for complaint in tqdm(list(df_clean[\"cleaned_complaints\"])):\n","    lemm_complaints.append(lemmatize(complaint))\n","df_clean[\"lemm_complaints\"] = lemm_complaints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOiDVvEIJrF0"},"outputs":[],"source":["df_clean.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kk7fc4DuJrF1"},"outputs":[],"source":["#Function to extract the POS tags \n","\n","def pos_tag(text):\n","  doc = nlp(text)\n","  out = []\n","  for token in doc:\n","    if token.tag_ == \"NN\":\n","      out.append(token.text)\n","  return \" \".join(out)\n","\n","pos_removed = [pos_tag(text) for text in tqdm(lemm_complaints)]\n","\n","\n","df_clean[\"complaint_POS_removed\"] =  pos_removed\n"," #this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjxfchvFJrF2"},"outputs":[],"source":["#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n","df_clean = df_clean[[\"complaint_what_happened\",\"lemm_complaints\",\"complaint_POS_removed\"]]\n","df_clean.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_7Un1AElJrF2"},"source":["## Exploratory data analysis to get familiar with the data.\n","\n","*   Visualising the data according to the 'Complaint' character length\n","*   Using a word cloud to find the top 40 words by frequency among all the articles after processing the text\n","*   Finding the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. ‘"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-zaqJF6JrF2"},"outputs":[],"source":["# Code to visualise the data according to the 'Complaint' character length\n","import seaborn as sb\n","import matplotlib.pyplot as plt\n","df_length = df_clean[[\"complaint_POS_removed\"]].copy()\n","df_length[\"char_length\"] = df_length[\"complaint_POS_removed\"].apply(lambda x: len(str(x)))\n","length_comp=list(df_length[\"char_length\"])\n","sb.boxplot(length_comp)\n","plt.show()\n","hist=sb.histplot(length_comp)\n","hist.set_xlim(0,4000)\n","plt.show()\n","df_length[\"char_length\"].describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Observation : The maximum character length of complaint is 12160 and the average complaint length is 385 characters. Most of the complaints are within 500 characters"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T9jD_6SeJrF3"},"source":["#### Finding the top 40 words by frequency among all the articles after processing the text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcfdvtfZJrF3"},"outputs":[],"source":["#Using a word cloud to find the top 40 words by frequency among all the articles after processing the text\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","\n","def show_wordcloud(data, title = None):\n","    stopwords = set(STOPWORDS)\n","    data_all=\" \".join(list(data))\n","    wordcloud = WordCloud(\n","        background_color='white',\n","        stopwords=stopwords,\n","        max_words=40,\n","        max_font_size=40, \n","        scale=3,\n","        random_state=1\n","    ).generate(data_all)\n","    \n","    text_dict=wordcloud.process_text(data_all)\n","    word_freq={k: v for k, v in sorted(text_dict.items(),reverse=True, key=lambda item: item[1])}\n","    print(\"The top frequent words with their frequency are : \",list(word_freq.items())[:40])\n","    \n","    fig = plt.figure(1, figsize=(12, 12))\n","    plt.axis('off')\n","    if title: \n","        fig.suptitle(title, fontsize=20)\n","        fig.subplots_adjust(top=2.3)\n","    plt.imshow(wordcloud)\n","    plt.show()\n","\n","df_clean[\"complaint_POS_removed\"] = df_clean[\"complaint_POS_removed\"].astype(str)\n","show_wordcloud(df_clean[\"complaint_POS_removed\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkSmc3UaJrF4"},"outputs":[],"source":["#Removing -PRON- from the text corpus\n","df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5DfCSbbmJrF4"},"source":["#### Finding the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mbk5DS5JrF4"},"outputs":[],"source":["#Code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean). \n","from nltk import ngrams,FreqDist\n","from collections import OrderedDict\n","\n","complaint_all=\" \".join(list(df_clean['Complaint_clean']))\n","print(\"Length of all complaints together\",len(complaint_all))\n","\n","def ngram_extract(data,n,top):\n","  ngram = ngrams(data.split(\" \"), n)\n","  #compute frequency distribution for all the bigrams in the text\n","  fdist = FreqDist(ngram)\n","  word_freq={k: v for k, v in sorted(fdist.items(),reverse=True, key=lambda item: item[1])}\n","  top_ngrams=list(word_freq.items())[:top]\n","  return OrderedDict(word_freq),top_ngrams\n","  \n","unigram_freq_dist,top_unigrams=ngram_extract(complaint_all,1,30)\n","print(f\"The top 30 unigrams with their frequency are : {top_unigrams}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YX7fedm1JrF8"},"outputs":[],"source":["#Printing the top 10 words in the unigram frequency\n","def extract_top_words_fdist(fdist,top):\n","    words=list(fdist.keys())[:top]\n","    words=[\" \".join(word) for word in words]\n","    return words\n","\n","top_10_unigrams=extract_top_words_fdist(unigram_freq_dist,10)\n","print(\"The top 10 words by unigram frequency are : \",top_10_unigrams)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aV7kD7w8JrF8"},"outputs":[],"source":["#Code to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean). \n","bigram_freq_dist,top_bigrams=ngram_extract(complaint_all,2,30)\n","print(f\"The top 30 bigrams with their frequency are : {top_bigrams}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPnMNIpyJrF9"},"outputs":[],"source":["#Printing top 10 words in the bigram frequency\n","top_10_bigrams=extract_top_words_fdist(bigram_freq_dist,10)\n","print(\"The top 10 words by bigram frequency are : \",top_10_bigrams)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xkh7vtbtJrF-"},"outputs":[],"source":["#Code to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean). \n","trigram_freq_dist,top_trigrams=ngram_extract(complaint_all,3,30)\n","print(f\"The top 30 bigrams with their frequency are : {top_trigrams}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REcVxNfvJrF-"},"outputs":[],"source":["#Printing the top 10 words in the trigram frequency\n","top_10_trigrams=extract_top_words_fdist(trigram_freq_dist,10)\n","print(\"The top 10 words by trigram frequency are : \",top_10_trigrams)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yUXzFji0JrF_"},"source":["#### The personal details of customer has been masked in the dataset with xxxx. Removing the masked text as this will be of no use for our analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKda-a_IJrF_"},"outputs":[],"source":["df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UIFk8fQJrF_"},"outputs":[],"source":["#All masked texts has been removed\n","df_clean.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k-I0k0QtJrGA"},"source":["## Feature Extraction\n","Converting the raw texts to a matrix of TF-IDF features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8fGwaCPJrGA"},"outputs":[],"source":["#Write your code here to initialise the TfidfVectorizer \n","tfidf = TfidfVectorizer(max_df = 0.95,min_df = 2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yYzD85nTJrGA"},"source":["#### Creating a document term matrix using fit_transform"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffzdDpp_JrGB"},"outputs":[],"source":["#Code to create the Document Term Matrix by transforming the complaints column present in df_clean.\n","dtm = tfidf.fit_transform(df_clean['Complaint_clean'])\n","dtm_df=pd.DataFrame(dtm.toarray(), columns=tfidf.get_feature_names_out())\n","print(\"The shape of document term matrix is \",dtm_df.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7Q9lwvNEJrGB"},"source":["## Topic Modelling using NMF\n","Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amLT4omWJrGB"},"outputs":[],"source":["from sklearn.decomposition import NMF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgd2A6bhJrGD"},"outputs":[],"source":["#Loading nmf_model with the n_components i.e 5\n","num_topics = 5\n","\n","nmf_model = NMF(n_components=num_topics,random_state=40)\n","W = nmf_model.fit_transform(dtm)  # Document-topic matrix\n","H = nmf_model.components_       # Topic-term matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPMDYbt_JrGE"},"outputs":[],"source":["#Fitting NMF model\n","nmf_model.fit(dtm)\n","len(tfidf.get_feature_names_out())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16kRfat5JrGE"},"outputs":[],"source":["#Printing the Top15 words for each of the topics\n","tot_words=15\n","words = np.array(tfidf.get_feature_names_out())\n","topic_words = pd.DataFrame(np.zeros((num_topics, tot_words)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n","                           columns=[f'Word {i + 1}' for i in range(tot_words)]).astype(str)\n","for i in range(num_topics):\n","    ix = H[i].argsort()[::-1][:tot_words]\n","    topic_words.iloc[i] = words[ix]\n","\n","topic_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OIT7LmFJrGF"},"outputs":[],"source":["#Creating the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n","#Topic 1 - Bank account services - (0)\n","#Topic 2 - Credit card / Prepaid card - (1)\n","#Topic 3 - Mortgages/loans - (2)\n","#Topic 4 - Theft/Dispute reporting - (3)\n","#Topic 5 - Others - (4)\n","\n","topic_mapping = {\n","    'Topic 1': 0,\n","    'Topic 2': 1,\n","    'Topic 3': 2,\n","    'Topic 4': 3,\n","    'Topic 5': 4,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peyYv-ORJrGF"},"outputs":[],"source":["#Assigning the best topic to each of the complaints in Topic Column\n","W = pd.DataFrame(W, columns=[f'Topic {i + 1}' for i in range(num_topics)])\n","W['Topic'] = W.apply(lambda x: topic_mapping.get(x.idxmax()), axis=1)\n","W[pd.notnull(W['Topic'])].head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLh_Gf3nJrGF"},"outputs":[],"source":["df_clean[\"Topic\"]=W[\"Topic\"]\n","df_clean.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQKpufSPJrGG"},"outputs":[],"source":["#Printing the first 5 Complaint for each of the Topics\n","df_clean_group=df_clean.groupby('Topic').head(5)\n","df_clean_group.sort_values('Topic')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(df_clean))"]},{"cell_type":"markdown","metadata":{"id":"piyLxzj6v07j"},"source":["#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n","* Bank Account services\n","* Credit card or prepaid card\n","* Theft/Dispute Reporting\n","* Mortgage/Loan\n","* Others"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWpwDG4RJrGG"},"outputs":[],"source":["Topic_names = {0:\"Bank account services\",1:\"Credit card / Prepaid card\",2:\"Mortgages/loans\",3:\"Theft/Dispute reporting\",4:\"Others\"}\n","#Replace Topics with Topic Names\n","df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2ULY5K6JrGG"},"outputs":[],"source":["df_clean.head(10)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7Mu0QBOcJrGH"},"source":["## Supervised model to predict any new complaints to the relevant Topics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_U8J3J8wJrGH"},"outputs":[],"source":["#Creating the dictionary again of Topic names and Topics\n","\n","Topic_names = { \"Bank account services\":0,\"Credit card / Prepaid card\":1,\"Mortgages/loans\":2,\"Theft/Dispute reporting\":3,\"Others\":4 }\n","#Replacing Topics with Topic Names\n","df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWIgJUkQJrGH"},"outputs":[],"source":["df_clean.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xx-FrbkWJrGH"},"outputs":[],"source":["#Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n","training_data=df_clean[[\"complaint_what_happened\",\"Topic\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVg2pa12JrGI"},"outputs":[],"source":["training_data.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"280Vbqk-7a8M"},"source":["#### Applying the supervised models on the training data created\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUlQpgkzJrGI"},"outputs":[],"source":["#Write your code to get the Vector count\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n","count_vect = CountVectorizer()\n","train_count = count_vect.fit_transform(training_data['complaint_what_happened'])\n","tfidf_model = TfidfTransformer()\n","#Write your code here to transform the word vector to tf-idf\n","X = tfidf_model.fit_transform(train_count)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uMU3vj6w-wqL"},"source":["Models Used :\n","* Logistic regression\n","* Decision Tree\n","* Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udLHpPsZJrGI"},"outputs":[],"source":["# Code to build any 3 models and evaluate them using the required metrics\n","y = training_data[[\"Topic\"]]\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2OznsObJrGP"},"outputs":[],"source":["# Building the models - \n","from sklearn.model_selection import GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifiers = [LogisticRegression(random_state=42),DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logistic = classifiers[0]\n","logistic.fit(X_train,y_train)\n","print(\"Logistic regression: \")\n","print(classification_report(y_test,logistic.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["decision = classifiers[1]\n","decision.fit(X_train,y_train)\n","print(\"Decision tree: \")\n","print(classification_report(y_test,decision.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random = classifiers[2]\n","random.fit(X_train,y_train)\n","print(\"Random forest: \")\n","print(classification_report(y_test,random.predict(X_test)))"]},{"cell_type":"markdown","metadata":{},"source":["### Selecting the best model :\n","#### Based on the evaluation metrics, Logistic regression performs the best with accuracy of 0.91 compared to Decision Tree(acc=0.76) & Random Forest(acc=0.82)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Testing the model with a custom test case using the best model**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_sentences = pd.DataFrame({\"complaint_what_happened\":[\"I want to check why my account is not having money, is it because my account is breached?\",\" The bill for my card is not paid, what is the due amount for this month for my card?\",\"What is the interest rate your bank offers for buying a house\",\" I want to enquire about opening a new account with your bank\",\"What is the best way to earn without job?\"]})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_test_sentences = count_vect.transform(test_sentences[\"complaint_what_happened\"])\n","X_test_sentences = tfidf_model.transform(X_test_sentences)\n","Topic_names = {0:\"Bank account services\",1:\"Credit card / Prepaid card\",2:\"Mortgages/loans\",3:\"Theft/Dispute reporting\",4:\"Others\"}\n","test_sentences[\"Topic\"]= logistic.predict(X_test_sentences)\n","test_sentences[\"Topic\"] = test_sentences[\"Topic\"].map(Topic_names)\n","test_sentences.head()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **Conclusion :**\n","\n","#### - We have used preprocessing to clean the data and then used topic modelling using NMF to create labels for the complaints\n","#### - We built classification models and found that Logistic regression is best suited with accuracy of 0.91\n","#### - We also tested on a custom case and found accurate results"]},{"cell_type":"markdown","metadata":{},"source":["# End of Notebook"]}],"metadata":{"colab":{"collapsed_sections":["T9jD_6SeJrF3","5DfCSbbmJrF4","yYzD85nTJrGA","piyLxzj6v07j","280Vbqk-7a8M"],"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"61d6566714c4bbac3dbe8b54cc0a5358529cb3ee416c2ac4526e2a6adbd07847"}}},"nbformat":4,"nbformat_minor":0}
